---
title: "PSY8712-Week12"
author: "Mackenzie R Nickle"
date: "2024-04-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Script Settings and Resources
```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(tidytext)
library(wordcloud)
library(topicmodels)
library(tm)
library(RedditExtractoR)
library(ldatuning)
library(dendextend)
library(textstem)
library(rJava)
library(qdap)
library(RWeka)
```


Data Import and Cleaning
```{r}
#getting urls
#reddit_thread_urls <- find_thread_urls(
 # subreddit= "IOPsychology",
 # sort_by= "new",
 # period= "year"
#) %>%
 # mutate(date_utc=ymd(date_utc))
#write_csv(reddit_thread_urls, file = "../data/reddit_thread_urls.csv")
#specifying urls for the past year
#thread_urls2 <- filter(reddit_thread_urls, timestamp > as.numeric(as.POSIXct(Sys.Date()-365)))
#nrow(thread_urls2)
#write_csv(thread_urls2, file="../data/thread_urls2.csv")
#getting content
#reddit_content <- get_thread_content(thread_urls2$url)
#saveRDS(reddit_content, file= "../data/reddit_content.RDS")
###making the reddit tibble
#week12_tbl <- tibble(
  #title = reddit_content$threads$title,
 # upvotes = reddit_content$threads$upvotes
#)
#write_csv(week12_tbl, file= "../data/week12_tbl.csv")

##reimporting just the previously created csv file for week12_tbl

week12_tbl <- read_csv("../data/week12_tbl.csv")
n <- nrow(week12_tbl)

##creating corpus

io_corpus_original <- VCorpus(VectorSource(week12_tbl$title)) #using data camp procedures

io_corpus <- io_corpus_original %>% 
  tm_map(content_transformer(str_to_lower)) %>% #making all words lowercase so I don't have to specify IO vs io
  tm_map(removePunctuation) %>% #removing punctuation so I don't have to deal with punctuation when getting rid of io psych 
  tm_map(removeNumbers) %>% #removing numbers because we don't need these for this analysis
  tm_map(removeWords, c("io", "iopsychology", "iopsychologist", "io psych", "riopsychology", "iopsychologists", "riopsychologists", "industrial organizational psychology", "industrial organizational psych", "io psychs", "psychology", "psych", "industrial organizational", "iop", "industrialorganizational")) %>% 
  tm_map(removeWords, stopwords("en")) %>% #removing normal english stopwords. did this after removing the io stuff because it was getting rid of i's 
  tm_map(stripWhitespace) %>%  #removing extra white space
  tm_map(content_transformer(replace_abbreviation)) %>% #replacing abbreviations
  tm_map(content_transformer(replace_contraction)) #replacing contractions
  
compare_them <- function(x,y){ #used x,y so can be used for other corpuses if needed
  case <- sample(1:n,1) #n being the number of rows (used nrows on week12_tbl for this but wanted the function to work outside of specific tibble)
  print(x[[case]]$content) #prints the content for the selected row for x
  print(y[[case]]$content)#prints the content for the selected row for y 
}

compare_them(io_corpus_original, io_corpus) #running the compare function to see differences between the original corpus and the preprocessed corpus. seems to be goodish

#creating DTM 
tokenizer <- function(x) NGramTokenizer(x, Weka_control(min= 1, max=2)) #creating tokenizer like data campl to include unigrams and bigrams
io_dtm <- DocumentTermMatrix(io_corpus, control= list(tokenize= tokenizer))

#making dtm a matrix then a tibble so I can view it
io_dtm_tbl <- io_dtm %>% as.matrix %>% as.tibble

#Making slim dtm
io_slim_dtm <- removeSparseTerms(io_dtm, 0.996) # used remove Sparse Terms, messed around with the percentage to get the correct ratio. .997 was 623:519 while .995 was 623:168

io_slim_dtm_tbl <- io_slim_dtm %>% as.matrix %>% as.tibble #ratio is between 2:1 to 3:1 because it's 623 to 250

tokenCounts <- apply(io_slim_dtm, 1, sum) #removing empty rows per lecture slides

original_rows <- rownames(io_slim_dtm) #used chatgpt to figure out how to save which rows would be removed
io_clean_dtm <- io_slim_dtm[tokenCounts > 0, ] #cleaning io_slim_dtm per lecture slides

removed_rows <- setdiff(original_rows, rownames(io_clean_dtm)) #code from chatgpt to see which rows were removed

print(removed_rows) #viewing which rows were removed

io_clean_dtm_tbl <- io_clean_dtm %>% as.matrix %>% as.tibble #making io_clean_dtm a matrix and a tibble so I can examine. Cleaning removed 54 observations. 

#using lda to categorize topics from io_slim_dtm

io_dtm_tune <- FindTopicsNumber(
  io_clean_dtm,
  topics = seq(2,15,1),
  metrics= c("Griffiths2004",
             "CaoJaun2009",
             "Arun2010",
             "Deveaud2014"),
  verbose=T
)
```



Visualization

Analysis
