---
title: "PSY8712-Week12"
author: "Mackenzie R Nickle"
date: "2024-04-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Script Settings and Resources
```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(tidytext)
library(wordcloud)
library(topicmodels)
library(tm)
library(RedditExtractoR)
library(ldatuning)
library(dendextend)
library(textstem)
library(rJava)
library(qdap)
library(RWeka)
library(caret)
library(parallel)
library(doParallel)
```


# Data Import and Cleaning

```{r}
#getting urls
#reddit_thread_urls <- find_thread_urls(
 # subreddit= "IOPsychology",
 # sort_by= "new",
 # period= "year"
#) %>%
 # mutate(date_utc=ymd(date_utc))
#write_csv(reddit_thread_urls, file = "../data/reddit_thread_urls.csv")
#specifying urls for the past year
#thread_urls2 <- filter(reddit_thread_urls, timestamp > as.numeric(as.POSIXct(Sys.Date()-365)))
#nrow(thread_urls2)
#write_csv(thread_urls2, file="../data/thread_urls2.csv")
#getting content
#reddit_content <- get_thread_content(thread_urls2$url)
#saveRDS(reddit_content, file= "../data/reddit_content.RDS")
###making the reddit tibble
#week12_tbl <- tibble(
  #title = reddit_content$threads$title,
 # upvotes = reddit_content$threads$upvotes
#)
#write_csv(week12_tbl, file= "../data/week12_tbl.csv")

##reimporting just the previously created csv file for week12_tbl

week12_tbl <- read_csv("../data/week12_tbl.csv")
n <- nrow(week12_tbl)
```

```{r}
##creating corpus

io_corpus_original <- VCorpus(VectorSource(week12_tbl$title)) #using data camp procedures

io_corpus <- io_corpus_original %>% 
  tm_map(content_transformer(str_to_lower)) %>% #making all words lowercase so I don't have to specify IO vs io
  tm_map(removePunctuation) %>% #removing punctuation so I don't have to deal with punctuation when getting rid of io psych 
  tm_map(removeNumbers) %>% #removing numbers because we don't need these for this analysis
  tm_map(removeWords, c("io", "iopsychology", "iopsychologist", "io psych", "riopsychology", "iopsychologists", "riopsychologists", "industrial organizational psychology", "industrial organizational psych", "io psychs", "psychology", "psych", "industrial organizational", "iop", "industrialorganizational")) %>% 
  tm_map(removeWords, stopwords("en")) %>% #removing normal english stopwords. did this after removing the io stuff because it was getting rid of i's 
  tm_map(stripWhitespace) %>%  #removing extra white space
  tm_map(content_transformer(replace_abbreviation)) %>% #replacing abbreviations
  tm_map(content_transformer(replace_contraction)) #replacing contractions
  
compare_them <- function(x,y){ #used x,y so can be used for other corpuses if needed
  case <- sample(1:n,1) #n being the number of rows (used nrows on week12_tbl for this but wanted the function to work outside of specific tibble)
  print(x[[case]]$content) #prints the content for the selected row for x
  print(y[[case]]$content)#prints the content for the selected row for y 
}

compare_them(io_corpus_original, io_corpus) #running the compare function to see differences between the original corpus and the preprocessed corpus. seems to be goodish
```

```{r}
#creating DTM 
tokenizer <- function(x) NGramTokenizer(x, Weka_control(min= 1, max=2)) #creating tokenizer like data campl to include unigrams and bigrams
io_dtm <- DocumentTermMatrix(io_corpus, control= list(tokenize= tokenizer))

#making dtm a matrix then a tibble so I can view it
io_dtm_tbl <- io_dtm %>% as.matrix %>% as_tibble

#Making slim dtm
io_slim_dtm <- removeSparseTerms(io_dtm, 0.996) # used remove Sparse Terms, messed around with the percentage to get the correct ratio. .997 was 623:519 while .995 was 623:168

io_slim_dtm_tbl <- io_slim_dtm %>% as.matrix %>% as_tibble #ratio is between 2:1 to 3:1 because it's 623 to 250

tokenCounts <- apply(io_slim_dtm, 1, sum) #removing empty rows per lecture slides

original_rows <- rownames(io_slim_dtm) #used chatgpt to figure out how to save which rows would be removed
io_clean_dtm <- io_slim_dtm[tokenCounts > 0, ] #cleaning io_slim_dtm per lecture slides

removed_rows <- setdiff(original_rows, rownames(io_clean_dtm)) #code from chatgpt to see which rows were removed

print(removed_rows) #viewing which rows were removed

io_clean_dtm_tbl <- io_clean_dtm %>% as.matrix %>% as_tibble #making io_clean_dtm a matrix and a tibble so I can examine. Cleaning removed 54 observations. 
```

```{r}
#using lda to categorize topics from io_slim_dtm
cluster <- makeCluster(4) #doing 4 because I had fatal errors at 5. 
registerDoParallel(cluster)
tuning <- FindTopicsNumber(
  io_clean_dtm,
  topics = seq(2,10,1), #changed from 15 to 10 from slides because it was causing fatal errors
  metrics= c("Griffiths2004",
             "CaoJuan2009",
             "Arun2010",
             "Deveaud2014"),
  verbose=T
) #copied this text from lecture slides to begin topic extraction

FindTopicsNumber_plot(tuning) #copied text from slides

stopCluster(cluster) #stopping parallelization
registerDoSEQ()
```

```{r}
##examining lda results with all code taken from lecture slides. the only edit was switching from 10 factors to 5 which I made because ten seemed like too many. 
lda_results <- LDA(io_clean_dtm, 5) 

lda_betas <- tidy(lda_results, matrix="beta")
lda_gammas <- tidy(lda_results, matrix= "gamma")

lda_betas %>% 
  group_by(topic) %>%
  top_n(5, beta) %>%
  arrange(topic, -beta) %>%
  View

lda_gammas %>% 
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(document= as.numeric(document)) %>%
  arrange(document) %>%
  View
```

```{r}
topics_tbl <- week12_tbl %>%
  mutate(doc_id = 1:623) %>% #creating doc_id from original data
  filter(!doc_id %in% removed_rows) %>% #removing rows that were removed when creating the topics
  mutate(doc_id = as.character(doc_id)) %>% #making doc_id character so I can merge with gamma
  full_join(lda_gammas, by= c("doc_id" = "document")) %>% #joining gamma results with original doc_ids
  rename(original=title, #renaming titles to original per assignment instructions 
         probability = gamma) %>% #renaming gamma to probability per assignment instructions
  select(doc_id, original, topic, probability) #putting in order
```
Questions:
1. Based on the beta matrix alone, the final topics the topic lists seem to map onto are:
  Topic 1: is related to education and careers 
  Topic 2: is positive descriptors like good and best 
  Topic 3: is related to learning generally (readings and discussions were in this group)
  They don't seem to map onto topics very well to be honest because jobs and careers were put into separate topics while research and graduate school were combined with careers. 
  
2. The lowest probability text was assigned to topic 5 and 3 and it seems to be that they were interested in studying at a graduate program which sort of matches topic 3 but should have been sorted into Topic 1 based on my interpretation. The highest probability was about a discussion of what literature people were reading which does seems to fitish onto my topic interpretation of learning generally. Neither one of these is an exact fit however, and the topics don't seem to fit that well. 

# Visualization
```{r}
#created wordcloud using lecture slides. did 25 words because 50 seemed like a lot and was hard to interpret. 
wordcloud(
  words = names(io_dtm_tbl),
  freq = colSums (io_dtm_tbl) ,
  max.words= 25,
  colors = brewer.pal(8, "Pastel2")
)
```
Based on the wordcloud, it seems that most people are looking for advice related to jobs, careers or working, or graduate school. 

# Analysis

making final_tbl
```{r}
#same as topics_tbl code, just with upvotes kept. 
final_tbl <- topics_tbl <- week12_tbl %>%
  mutate(doc_id = 1:623) %>% #creating doc_id from original data
  filter(!doc_id %in% removed_rows) %>% #removing rows that were removed when creating the topics
  mutate(doc_id = as.character(doc_id)) %>% #making doc_id character so I can merge with gamma
  full_join(lda_gammas, by= c("doc_id" = "document")) %>% #joining gamma results with original doc_ids
  rename(original=title, #renaming titles to original per assignment instructions 
         probability = gamma) %>% #renaming gamma to probability per assignment instructions
  select(doc_id, original, upvotes, topic, probability) #putting in order
```

machine learning analysis
```{r}
#copied code from the machine learning project. used lm because the other ones were not working. 
holdout_indices <- createDataPartition(final_tbl$upvotes,
                                       p= .25,
                                       list= T)$Resample1
test_tbl <- final_tbl[holdout_indices,]
training_tbl <- final_tbl[-holdout_indices, ]

training_folds <- createFolds(training_tbl$upvotes)

model_lm <- train(
  upvotes ~ topic,
  training_tbl,
  method ="lm",
  na.action = na.pass,
  preProcess = "medianImpute",
  trControl = trainControl (method= "cv",
                            number = 10,
                            verboseIter = T,
                            indexOut = training_folds)
)

model_lm
cv_lm <- max(model_lm$results$Rsquared)
holdout_m1 <- cor(
  predict(model_lm, test_tbl, na.action = na.pass),
  test_tbl$upvotes
)^2

```

cv_lm = .003
holdout_m1 = .002
